{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 2: Stock Portfolio Optimization - Assignment 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Athanasakis - Fragkogiannis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk #loads standard python GUI libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tkinter import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment for Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1\n"
     ]
    }
   ],
   "source": [
    "print(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment*for Question 2* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the environment\n",
    "# We are modeling this environment using 8 states in the format: {stock_currently_holding,state_of_stock_1,state_of_stock_2}\n",
    "\n",
    "action_keep = 0     # keep the same stock \n",
    "action_switch = 1   # switch to the other stock\n",
    "\n",
    "\n",
    "P = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01),    # {2,L,L}\n",
    "            (1/20, 5, +0.05),    # {2,L,H}\n",
    "            (9/20, 6, +0.01),    # {2,H,L}\n",
    "            (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01),    # {2,L,L}\n",
    "            (9/20, 5, +0.05),    # {2,L,H}\n",
    "            (1/20, 6, +0.01),    # {2,H,L}\n",
    "            (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01),    # {2,L,L}\n",
    "            (1/20, 5, +0.05),    # {2,L,H}\n",
    "            (9/20, 6, +0.01),    # {2,H,L}\n",
    "            (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01),    # {2,L,L}\n",
    "            (9/20, 5, +0.05),    # {2,L,H}\n",
    "            (1/20, 6, +0.01),    # {2,H,L}\n",
    "            (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02),  # {1,L,L}\n",
    "            (9/20, 1, -0.02),  # {1,L,H}\n",
    "            (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "            (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "reward_values = [-0.02, -0.02, 0.1, 0.1, 0.01, 0.05, 0.01, 0.05]\n",
    "reward = np.array(reward_values)\n",
    "holes = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the environment for Question 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Policy Iteration Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100000 is out of bounds for axis 1 with size 100000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 69\u001b[0m\n\u001b[0;32m     63\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m###################### Question 2 ###########################\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m V_opt,P_opt \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# The following implements the GUI if you are going to use it on your own PC. It will not run as is on Colab. \u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# I would recommend to maintain some sort of simple GUI like this for your own implementations as well...it might help you. But it is not mandatory\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# If you want to use in Colab, comment out the code from this point on.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mP_to_text\u001b[39m(a):\n",
      "Cell \u001b[1;32mIn[22], line 50\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(P, gamma, epsilon)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     old_pi \u001b[38;5;241m=\u001b[39m {s: pi(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(P))}  \u001b[38;5;66;03m#keep the old policy to compare with new\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m#evaluate latest policy --> you receive its converged value function\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     pi \u001b[38;5;241m=\u001b[39m policy_improvement(V,P,gamma)          \u001b[38;5;66;03m#get a better policy using the value function of the previous one just calculated \u001b[39;00m\n\u001b[0;32m     53\u001b[0m     t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[1;34m(pi, P, gamma, epsilon)\u001b[0m\n\u001b[0;32m     23\u001b[0m     prev_V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;66;03m#freeze the new values (to be used as the next V(s'))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mVplot\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m prev_V  \u001b[38;5;66;03m# accounting for GUI  \u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m V\n",
      "\u001b[1;31mIndexError\u001b[0m: index 100000 is out of bounds for axis 1 with size 100000"
     ]
    }
   ],
   "source": [
    "# The next few lines are mostly for accounting\n",
    "Tmax = 100000\n",
    "size = len(P)\n",
    "n = m = np.sqrt(size)\n",
    "print(size)\n",
    "Vplot = np.zeros((size,Tmax)) #these keep track how the Value function evolves, to be used in the GUI\n",
    "Pplot = np.zeros((size,Tmax)) #these keep track how the Policy evolves, to be used in the GUI\n",
    "t = 0\n",
    "\n",
    "\n",
    "#this one is generic to be applied in many AI gym compliant environments\n",
    "\n",
    "def policy_evaluation(pi, P, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
    "    t = 0   #there's more elegant ways to do this\n",
    "    prev_V = np.zeros(len(P)) # use as \"cost-to-go\", i.e. for V(s')\n",
    "    while True:\n",
    "        V = np.zeros(len(P)) # current value function to be learnerd\n",
    "        for s in range(len(P)):  # do for every state\n",
    "            for prob, next_state, reward in P[s][pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state])\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one; \n",
    "            break # if yes, finish loop\n",
    "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "        t += 1\n",
    "        Vplot[:,t] = prev_V  # accounting for GUI  \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(V, P, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64) #create a Q value array\n",
    "    for s in range(len(P)):        # for every state in the environment/model\n",
    "        for a in range(len(P[s])):  # and for every action in that state\n",
    "            for prob, next_state, reward in P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve) \n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state])\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
    "    # lambda is a \"fancy\" way of creating a function without formally defining it (e.g. simply to return, as here...or to use internally in another function)\n",
    "    # you can implement this in a much simpler way, by using just a few more lines of code -- if this command is not clear, I suggest to try coding this yourself\n",
    "    \n",
    "    return new_pi\n",
    "\n",
    "# policy iteration is simple, it will call alternatively policy evaluation then policy improvement, till the policy converges.\n",
    "\n",
    "def policy_iteration(P, gamma = 1.0, epsilon = 1e-10):\n",
    "    t = 0\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state  \n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
    "    \n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))}  #keep the old policy to compare with new\n",
    "        V = policy_evaluation(pi,P,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "        pi = policy_improvement(V,P,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "        t += 1\n",
    "        Pplot[:,t]= [pi(s) for s in range(len(P))]  #keep track of the policy evolution\n",
    "        Vplot[:,t] = V                              #and the value function evolution (for the GUI)\n",
    "    \n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "            break\n",
    "    print('converged after %d iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    return V,pi\n",
    "\n",
    "n = 4\n",
    "m = 4\n",
    "\n",
    "\n",
    "#############################################################\n",
    "###################### Question 2 ###########################\n",
    "\n",
    "V_opt,P_opt = policy_iteration(P,gamma = 1)\n",
    "\n",
    "\n",
    "\n",
    "# The following implements the GUI if you are going to use it on your own PC. It will not run as is on Colab. \n",
    "# I would recommend to maintain some sort of simple GUI like this for your own implementations as well...it might help you. But it is not mandatory\n",
    "# If you want to use in Colab, comment out the code from this point on.\n",
    "\n",
    "\n",
    "def P_to_text(a):\n",
    "    if a == 0: return 'K' \n",
    "    if a == 1: return 'S'\n",
    "    # if a == 2: return 'R'\n",
    "    # if a == 3: return 'U'\n",
    "    \n",
    "    \n",
    "for s in range(len(P)):\n",
    "    print(P_opt(s))\n",
    "\n",
    "frame_text_V = tk.Frame()\n",
    "frame_V = tk.Frame(highlightbackground=\"blue\", highlightthickness=2)\n",
    "frame_text_P = tk.Frame()\n",
    "frame_P = tk.Frame(highlightbackground=\"green\", highlightthickness=2)\n",
    "frame_text_R = tk.Frame()\n",
    "frame_R = tk.Frame(highlightbackground=\"red\", highlightthickness=2)\n",
    "\n",
    "\n",
    "\n",
    "def submit():\n",
    "    iter = int(e.get())\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n):\n",
    "        cols = []\n",
    "        for j in range(m):\n",
    "#            e = Entry(relief=GROOVE, master = frame_V)\n",
    "            e2 = tk.Label(master = frame_V, text = ( '%f'  %(Vplot[i*m+j,iter])), font=(\"Arial\", 14))\n",
    "            e2.grid(row=i, column=j, sticky=N+S+E+W, padx=10, pady = 10)   \n",
    "#            e.insert(END, '%f'  %(v[i,j]))\n",
    "            cols.append(e2)    \n",
    "        rows.append(cols)\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for i in range(n):\n",
    "        cols = []\n",
    "        for j in range(m):\n",
    "#            e = Entry(relief=GROOVE, master = frame_V)\n",
    "            if i*m+j in holes:\n",
    "                e3 = tk.Label(master = frame_P, text = 'H', font=(\"Arial\", 18))\n",
    "            else:\n",
    "                e3 = tk.Label(master = frame_P, text = P_to_text((Pplot[i*m+j,iter])), font=(\"Arial\", 14))\n",
    "            e3.grid(row=i, column=j, sticky=N+S+E+W, padx=10, pady = 10)            \n",
    "#            e.insert(END, '%f'  %(v[i,j]))\n",
    "            cols.append(e3)\n",
    "        rows.append(cols)\n",
    "         \n",
    "rows = []\n",
    "    \n",
    "for i in range(n):\n",
    "    cols = []\n",
    "    for j in range(m):\n",
    "        e2 = tk.Label(master = frame_R, text = ( '%f'  %(reward[i*m+j])), font=(\"Arial\", 14))\n",
    "        e2.grid(row=i, column=j, sticky=N+S+E+W, padx=10, pady = 10)\n",
    "#        e2.insert(END, '%f'  %(r[i,j]))\n",
    "        cols.append(e2)\n",
    "    rows.append(cols)\n",
    "    \n",
    "    \n",
    "\n",
    "label_V = tk.Label(master=frame_text_V, text=\"Value Function at Iteration:\", font=(\"Arial\", 18))\n",
    "label_V.pack()\n",
    "iter_btn=tk.Button(frame_text_V,text = 'Submit', command = submit, font=(\"Arial\", 18))\n",
    "iter_btn.pack()\n",
    "e = Entry(relief=GROOVE, master = frame_text_V, font=(\"Arial\", 18))\n",
    "e.pack()\n",
    "\n",
    "\n",
    "\n",
    "label_P = tk.Label(master=frame_text_P, text=\"Optimal Policy:\", font=(\"Arial\", 18))\n",
    "label_P.pack()\n",
    "\n",
    "label_R = tk.Label(master=frame_text_R, text=\"Reward Function:\", font=(\"Arial\", 18))\n",
    "label_R.pack()\n",
    "\n",
    "frame_text_V.pack()\n",
    "frame_V.pack()\n",
    "frame_text_P.pack()\n",
    "frame_P.pack()\n",
    "frame_text_R.pack()\n",
    "frame_R.pack()\n",
    "\n",
    "\n",
    "mainloop()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
